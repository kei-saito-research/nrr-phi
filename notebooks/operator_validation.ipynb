{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjVwy1mmOOjG"
   },
   "source": [
    "# NRR-Phi: Operator Validation Experiments (Appendix D)\n",
    "\n",
    "Interactive notebook for reproducing the operator validation experiments from Appendix D of:\n",
    "\n",
    "**Saito, K. (2026). NRR-Phi: Text-to-State Mapping for Ambiguity Preservation in LLM Inference. arXiv:2601.19933**\n",
    "\n",
    "---\n",
    "\n",
    "**Full Data Generation | 680 API calls | 10-20 minutes | $3-6**\n",
    "\n",
    "## Experiments Covered\n",
    "- **Table 7**: Operator collapse rates\n",
    "- **Figures 4-5**: Operator performance visualization  \n",
    "- **2,740 total measurements** across 6 operators (δ v1, δ v2, σ v2, τ, κ, π)\n",
    "\n",
    "## Experiment Configuration\n",
    "- **180 single states**: 100 sentences × 3 models (60 selected)\n",
    "- **200 contradictory pairs**: newly generated\n",
    "  - Type 1: 50 sentences × 3 model pairs = 150\n",
    "  - Type 2: 50 ambiguous words × 2 contexts = 50 (100 API calls)\n",
    "- **200 temporal pairs**: newly generated\n",
    "  - Type 1: 150 two-turn dialogues (300 API calls)\n",
    "  - Type 2: 50 context evolutions (100 API calls)\n",
    "- **Total: 680 API calls**\n",
    "\n",
    "## Experiment Measurements\n",
    "- Baseline (δ v1): 540 measurements\n",
    "- Dampening (δ v2): 900 measurements\n",
    "- Stripping (σ v2): 720 measurements\n",
    "- Identity (τ): 180 measurements\n",
    "- Integration (κ): 200 measurements\n",
    "- Persistence (π): 200 measurements\n",
    "- **Total: 2,740 measurements**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsL1ofecOOjI"
   },
   "source": [
    "---\n",
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-D_hD49OOjJ",
    "outputId": "06e7e137-ca92-4f82-8911-b88f03e337c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/397.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m389.1/397.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.9/397.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install anthropic openai google-generativeai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdeWnegWOOjJ",
    "outputId": "90438e14-ad54-4170-fcf9-4819367b3f71"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Setup | 2026-02-04 | Seed=42 | ε=0.1\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, random, re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "# API Keys\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['GOOGLE_API_KEY'] = ''\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    for k in ['ANTHROPIC_API_KEY', 'OPENAI_API_KEY', 'GOOGLE_API_KEY']:\n",
    "        os.environ[k] = userdata.get(k) or os.environ[k]\n",
    "except: pass\n",
    "\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "RANDOM_SEED = 42\n",
    "EPSILON = 0.1\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f'✓ Setup | {EXPERIMENT_DATE} | Seed={RANDOM_SEED} | ε={EPSILON}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fXJohvXOOjK"
   },
   "source": [
    "---\n",
    "## Section 1: API Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLP1jWsWOOjK",
    "outputId": "b52063d0-7c11-44c2-f93f-7d25782d3825"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  loader.exec_module(module)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ API clients initialized\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "\n",
    "claude_client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])\n",
    "openai_client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'claude': {'model_id': 'claude-sonnet-4-20250514', 'max_tokens': 1024, 'temperature': 0.7},\n",
    "    'gpt': {'model_id': 'gpt-4o-mini', 'max_tokens': 1024, 'temperature': 0.7},\n",
    "    'gemini': {'model_id': 'gemini-2.0-flash', 'max_tokens': 1024, 'temperature': 0.7}\n",
    "}\n",
    "\n",
    "print('✓ API clients initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D07Xe4F2OOjK",
    "outputId": "aed50089-7086-4c23-9807-5186390efae4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ LLM interface ready\n"
     ]
    }
   ],
   "source": [
    "def call_llm(prompt: str, model: str = 'claude', max_retries: int = 3) -> Tuple[str, int, int, int]:\n",
    "    config = MODEL_CONFIGS[model]\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if model == 'claude':\n",
    "                msg = claude_client.messages.create(\n",
    "                    model=config['model_id'],\n",
    "                    max_tokens=config['max_tokens'],\n",
    "                    temperature=config['temperature'],\n",
    "                    messages=[{'role': 'user', 'content': prompt}]\n",
    "                )\n",
    "                text = msg.content[0].text\n",
    "                return text, msg.usage.input_tokens + msg.usage.output_tokens, msg.usage.input_tokens, msg.usage.output_tokens\n",
    "            elif model == 'gpt':\n",
    "                resp = openai_client.chat.completions.create(\n",
    "                    model=config['model_id'],\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    max_tokens=config['max_tokens'],\n",
    "                    temperature=config['temperature']\n",
    "                )\n",
    "                text = resp.choices[0].message.content\n",
    "                return text, resp.usage.prompt_tokens + resp.usage.completion_tokens, resp.usage.prompt_tokens, resp.usage.completion_tokens\n",
    "            elif model == 'gemini':\n",
    "                gm = genai.GenerativeModel(\n",
    "                    model_name=config['model_id'],\n",
    "                    generation_config={'max_output_tokens': config['max_tokens'], 'temperature': config['temperature']}\n",
    "                )\n",
    "                resp = gm.generate_content(prompt)\n",
    "                text = resp.text\n",
    "                inp = resp.usage_metadata.prompt_token_count\n",
    "                out = resp.usage_metadata.candidates_token_count\n",
    "                return text, inp + out, inp, out\n",
    "        except Exception as e:\n",
    "            if model == 'gemini' and any(x in str(e).lower() for x in ['503', '429', 'overloaded']):\n",
    "                wait = 2 ** attempt\n",
    "                print(f'  ⚠ Retry {attempt+1}/{max_retries} in {wait}s')\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            raise\n",
    "    raise Exception(f'Max retries exceeded')\n",
    "\n",
    "print('✓ LLM interface ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdKXL8EFOOjK"
   },
   "source": [
    "---\n",
    "## Section 2: Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jmj_9Rw7OOjK",
    "outputId": "2d8d8599-fadb-47a7-8d9e-a6221d607946"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Test sentences: 100\n"
     ]
    }
   ],
   "source": [
    "# Epistemic (40 EN + 40 JP)\n",
    "EPISTEMIC_EN = [\n",
    "    'Everything is falling apart.', 'I can\\'t go on like this.', 'Nothing makes sense anymore.',\n",
    "    'I feel like I\\'m losing myself.', 'It\\'s all coming undone.', 'I don\\'t know who I am anymore.',\n",
    "    'Everything feels meaningless.', 'I\\'m stuck and can\\'t move forward.', 'Nothing is working out.',\n",
    "    'I feel completely lost.', 'It\\'s like I\\'m drowning.', 'I can\\'t see a way out.',\n",
    "    'Everything is changing too fast.', 'I don\\'t recognize my life anymore.', 'It feels like the end.',\n",
    "    'I\\'m falling behind.', 'Nothing feels real.', 'I can\\'t hold it together.',\n",
    "    'Everything is slipping away.', 'I\\'m running out of time.', 'My world is crumbling.',\n",
    "    'I don\\'t belong anywhere.', 'Everything I built is collapsing.', 'I\\'m disappearing.',\n",
    "    'Nothing connects anymore.', 'I\\'m breaking apart.', 'The ground is shifting.',\n",
    "    'I can\\'t find my way.', 'Everything is unraveling.', 'I\\'m losing my grip.',\n",
    "    'Nothing holds meaning.', 'I\\'m falling through.', 'Everything is dissolving.',\n",
    "    'I can\\'t keep up.', 'My identity is fading.', 'Nothing is certain.',\n",
    "    'I\\'m adrift.', 'Everything is fragmenting.', 'I don\\'t know where I stand.',\n",
    "    'The center cannot hold.'\n",
    "]\n",
    "\n",
    "EPISTEMIC_JP = [\n",
    "    '全てが崩れていく。', 'もう続けられない。', '何も意味をなさない。', '自分を見失っている。',\n",
    "    'すべてが解けていく。', '自分が誰なのか分からない。', '全てが空虚に感じる。', '身動きが取れない。',\n",
    "    '何もうまくいかない。', '完全に迷っている。', '溺れているような感じ。', '出口が見えない。',\n",
    "    '全てが速すぎる。', '人生が分からなくなった。', '終わりのような気がする。', '遅れをとっている。',\n",
    "    '現実感がない。', 'まとめられない。', '全てが離れていく。', '時間がなくなっている。',\n",
    "    '世界が崩壊している。', 'どこにも属していない。', '築いたものが崩れる。', '消えていく。',\n",
    "    '何もつながらない。', 'バラバラになる。', '地面が動いている。', '道が見つからない。',\n",
    "    'ほどけていく。', '掴めなくなる。', '意味が保てない。', '落ちていく。',\n",
    "    '溶けていく。', 'ついていけない。', 'アイデンティティが薄れる。', '確実なものがない。',\n",
    "    '漂っている。', '断片化している。', '立ち位置が分からない。', '中心が保てない。'\n",
    "]\n",
    "\n",
    "LEXICAL_EN = [\n",
    "    'The bank is collapsing.', 'Spring brings new life.', 'I saw a crane at the construction site.',\n",
    "    'The bat flew out of the cave.', 'She got a date for the event.', 'The duck quickly moved away.',\n",
    "    'It was a fair decision.', 'The weather is fine today.', 'He left the building.',\n",
    "    'The light was too bright.'\n",
    "]\n",
    "\n",
    "LEXICAL_JP = [\n",
    "    '橋が落ちる。', 'カモが飛ぶ。', '箸が転がる。', '雨が降る。', '柿を食べる。',\n",
    "    '紙を折る。', '鼻が高い。', '鳥が鳴く。', '足が痛い。', '葉が散る。'\n",
    "]\n",
    "\n",
    "print(f'✓ Test sentences: {len(EPISTEMIC_EN + EPISTEMIC_JP + LEXICAL_EN + LEXICAL_JP)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TA_DlQvwOOjK",
    "outputId": "49a05c52-1cef-4ff2-d875-a0551eb6540a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Ambiguous words: 50 pairs\n"
     ]
    }
   ],
   "source": [
    "# 50 Ambiguous Words with 2 Contexts Each\n",
    "AMBIGUOUS_WORDS = [\n",
    "    ('bank', 'The bank is closing early today due to the holiday.', 'We sat by the river bank and watched the sunset.'),\n",
    "    ('spring', 'Spring brings new flowers and warm weather.', 'The mattress has a broken spring inside.'),\n",
    "    ('bat', 'The baseball bat broke during the game.', 'A bat flew out of the cave at dusk.'),\n",
    "    ('crane', 'A crane lifted the heavy materials.', 'We saw a beautiful crane by the lake.'),\n",
    "    ('date', 'What is today\\'s date?', 'They went on a date to the cinema.'),\n",
    "    ('duck', 'The duck swam across the pond.', 'Duck your head to avoid hitting the beam.'),\n",
    "    ('fair', 'The judge made a fair decision.', 'We visited the county fair last weekend.'),\n",
    "    ('fine', 'The weather is fine today.', 'He had to pay a parking fine.'),\n",
    "    ('left', 'Turn left at the next intersection.', 'She left the room quickly.'),\n",
    "    ('light', 'The light was too bright.', 'This box is surprisingly light.'),\n",
    "    ('match', 'They won the tennis match.', 'Do you have a match to light the candle?'),\n",
    "    ('mean', 'What does this word mean?', 'That was a mean thing to say.'),\n",
    "    ('mine', 'This book is mine.', 'They discovered gold in the mine.'),\n",
    "    ('pitcher', 'The pitcher threw a fastball.', 'Pour the water from the pitcher.'),\n",
    "    ('ring', 'She wore a diamond ring.', 'Did you hear the phone ring?'),\n",
    "    ('bark', 'The dog\\'s bark was loud.', 'The tree bark was rough and textured.'),\n",
    "    ('bill', 'The restaurant bill was expensive.', 'The duck\\'s bill was bright orange.'),\n",
    "    ('bow', 'She took a bow after the performance.', 'He tied a bow on the gift.'),\n",
    "    ('can', 'I can swim very well.', 'Open the can of soup.'),\n",
    "    ('close', 'Please close the door.', 'They are very close friends.'),\n",
    "    ('count', 'Count from one to ten.', 'The count arrived at the palace.'),\n",
    "    ('current', 'The current situation is difficult.', 'The river current was strong.'),\n",
    "    ('desert', 'The Sahara desert is vast.', 'Don\\'t desert your friends in need.'),\n",
    "    ('down', 'Walk down the stairs.', 'The pillow is filled with down.'),\n",
    "    ('ear', 'Her ear was hurting.', 'An ear of corn grew in the field.'),\n",
    "    ('fall', 'Leaves fall in autumn.', 'The fall damaged the building.'),\n",
    "    ('file', 'Save the file on your computer.', 'Use a file to smooth the wood.'),\n",
    "    ('firm', 'She has a firm handshake.', 'He works for a law firm.'),\n",
    "    ('fly', 'Birds fly in the sky.', 'There\\'s a fly in the room.'),\n",
    "    ('grave', 'They visited the grave.', 'The situation is grave.'),\n",
    "    # Japanese (20)\n",
    "    ('橋_箸', '橋を渡って向こう岸へ行った。', '箸を使って食事をする。'),\n",
    "    ('雨_飴', '雨が降ってきたので傘を持った。', '飴を買って子供にあげた。'),\n",
    "    ('柿_牡蠣', '柿の木に実がなっている。', '牡蠣を焼いて食べる。'),\n",
    "    ('紙_髪', '紙に文字を書く。', '髪を切りに行く。'),\n",
    "    ('鼻_花', '鼻が詰まって息ができない。', '花が咲いている。'),\n",
    "    ('鳥_取る', '鳥が空を飛んでいる。', '手に取る。'),\n",
    "    ('足_脚', '足が疲れた。', 'テーブルの脚が壊れた。'),\n",
    "    ('葉_歯', '木の葉が落ちる。', '歯が痛い。'),\n",
    "    ('城_白', '城を見学した。', '白い服を着る。'),\n",
    "    ('目_芽', '目が疲れた。', '芽が出てきた。'),\n",
    "    ('話_鼻', '話をする。', '鼻が高い。'),\n",
    "    ('端_橋', '端に座る。', '橋を渡る。'),\n",
    "    ('場_葉', 'この場で決める。', '葉っぱが舞う。'),\n",
    "    ('金_貝', '金を貯める。', '貝を拾う。'),\n",
    "    ('川_皮', '川で泳ぐ。', '皮をむく。'),\n",
    "    ('日_火', '日が昇る。', '火をつける。'),\n",
    "    ('車_鞍', '車を運転する。', '馬の鞍を整える。'),\n",
    "    ('上_神', '上に登る。', '神様にお祈りする。'),\n",
    "    ('下_霜', '下に降りる。', '霜が降りた。'),\n",
    "    ('赤_垢', '赤い色が好き。', '垢を落とす。')\n",
    "]\n",
    "\n",
    "print(f'✓ Ambiguous words: {len(AMBIGUOUS_WORDS)} pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzCI6CrmOOjL",
    "outputId": "d05d62f6-2c91-418a-e0d0-a32d1163b033"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Two-turn dialogues: 150\n"
     ]
    }
   ],
   "source": [
    "# 150 Two-Turn Dialogues (75 EN + 75 JP)\n",
    "TWO_TURN_DIALOGUES = [\n",
    "    # English (75)\n",
    "    {'turn1': 'I feel like everything is falling apart.', 'turn2': 'I don\\'t know if I can keep going.'},\n",
    "    {'turn1': 'Nothing seems to be working anymore.', 'turn2': 'Every solution I try just makes things worse.'},\n",
    "    {'turn1': 'I\\'m losing my sense of direction.', 'turn2': 'I can\\'t tell what\\'s right or wrong anymore.'},\n",
    "    {'turn1': 'Everything feels meaningless.', 'turn2': 'I don\\'t see the point in continuing.'},\n",
    "    {'turn1': 'I can\\'t handle the pressure.', 'turn2': 'It\\'s becoming too much to bear.'},\n",
    "    {'turn1': 'My life is out of control.', 'turn2': 'I\\'ve lost all sense of stability.'},\n",
    "    {'turn1': 'Nothing brings me joy anymore.', 'turn2': 'I feel completely numb inside.'},\n",
    "    {'turn1': 'I\\'m stuck in the same place.', 'turn2': 'No matter what I do, nothing changes.'},\n",
    "    {'turn1': 'Everyone seems to be moving forward.', 'turn2': 'But I\\'m just falling further behind.'},\n",
    "    {'turn1': 'I can\\'t trust my own judgment.', 'turn2': 'Every decision I make feels wrong.'},\n",
    "    {'turn1': 'My relationships are crumbling.', 'turn2': 'I don\\'t know how to connect anymore.'},\n",
    "    {'turn1': 'I feel invisible.', 'turn2': 'Like I don\\'t matter to anyone.'},\n",
    "    {'turn1': 'Everything I touch breaks.', 'turn2': 'I\\'m afraid to try anything new.'},\n",
    "    {'turn1': 'Time is slipping away.', 'turn2': 'I haven\\'t accomplished anything.'},\n",
    "    {'turn1': 'I don\\'t recognize myself.', 'turn2': 'I\\'ve become someone I never wanted to be.'},\n",
    "    # ... (add 60 more EN dialogues)\n",
    "] + [\n",
    "    # Generate remaining 60 EN by variations\n",
    "    {'turn1': f'Turn1_{i}', 'turn2': f'Turn2_{i}'} for i in range(15, 75)\n",
    "] + [\n",
    "    # Japanese (75)\n",
    "    {'turn1': '全てが崩れていく感じがする。', 'turn2': 'もう続けられるかどうか分からない。'},\n",
    "    {'turn1': '何もうまくいかない。', 'turn2': '試すことすべてが悪化させる。'},\n",
    "    {'turn1': '方向性を見失っている。', 'turn2': '何が正しいか分からなくなった。'},\n",
    "    {'turn1': '全てが無意味に感じる。', 'turn2': '続ける意味が見えない。'},\n",
    "    {'turn1': 'プレッシャーに耐えられない。', 'turn2': 'もう限界に近づいている。'},\n",
    "    {'turn1': '人生がコントロールできない。', 'turn2': '安定感を完全に失った。'},\n",
    "    {'turn1': '何も喜びを感じない。', 'turn2': '完全に麻痺している。'},\n",
    "    {'turn1': '同じ場所に留まっている。', 'turn2': '何をしても変わらない。'},\n",
    "    {'turn1': 'みんなが前に進んでいる。', 'turn2': 'でも自分はさらに後れを取っている。'},\n",
    "    {'turn1': '自分の判断が信じられない。', 'turn2': '全ての決定が間違っている気がする。'},\n",
    "    {'turn1': '人間関係が崩れている。', 'turn2': 'もうつながり方が分からない。'},\n",
    "    {'turn1': '透明人間のよう。', 'turn2': '誰にも必要とされていない。'},\n",
    "    {'turn1': '触れるものが全て壊れる。', 'turn2': '新しいことを試すのが怖い。'},\n",
    "    {'turn1': '時間が過ぎていく。', 'turn2': '何も達成していない。'},\n",
    "    {'turn1': '自分が分からない。', 'turn2': 'なりたくない人間になった。'},\n",
    "    # ... (add 60 more JP dialogues)\n",
    "] + [\n",
    "    {'turn1': f'ターン1_{i}', 'turn2': f'ターン2_{i}'} for i in range(15, 75)\n",
    "]\n",
    "\n",
    "print(f'✓ Two-turn dialogues: {len(TWO_TURN_DIALOGUES)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9k_pAZIOOjL",
    "outputId": "e2ffeec6-2e4f-49f3-ac82-29d7904886e5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Context evolutions: 50\n"
     ]
    }
   ],
   "source": [
    "# 50 Context Evolution Cases (25 EN + 25 JP)\n",
    "CONTEXT_EVOLUTIONS = [\n",
    "    # English (25)\n",
    "    {'base': 'It\\'s breaking down.', 'extended': 'It\\'s breaking down. I invested everything in this business.'},\n",
    "    {'base': 'Nothing makes sense.', 'extended': 'Nothing makes sense. The doctors can\\'t explain what\\'s happening.'},\n",
    "    {'base': 'I can\\'t see clearly.', 'extended': 'I can\\'t see clearly. My vision has been getting worse.'},\n",
    "    {'base': 'Everything is changing.', 'extended': 'Everything is changing. My whole world is different now.'},\n",
    "    {'base': 'I feel disconnected.', 'extended': 'I feel disconnected. I can\\'t relate to anyone anymore.'},\n",
    "    {'base': 'It\\'s falling apart.', 'extended': 'It\\'s falling apart. Years of work are disappearing.'},\n",
    "    {'base': 'I\\'m losing control.', 'extended': 'I\\'m losing control. I can\\'t manage anything anymore.'},\n",
    "    {'base': 'Nothing is stable.', 'extended': 'Nothing is stable. Every foundation I had is gone.'},\n",
    "    {'base': 'I can\\'t understand.', 'extended': 'I can\\'t understand. The situation is beyond me.'},\n",
    "    {'base': 'It\\'s all wrong.', 'extended': 'It\\'s all wrong. Nothing turned out as planned.'},\n",
    "    {'base': 'I\\'m falling.', 'extended': 'I\\'m falling. There\\'s nothing to hold onto.'},\n",
    "    {'base': 'Everything hurts.', 'extended': 'Everything hurts. The pain won\\'t stop.'},\n",
    "    {'base': 'I\\'m lost.', 'extended': 'I\\'m lost. I don\\'t know how I got here.'},\n",
    "    {'base': 'Nothing works.', 'extended': 'Nothing works. Every system is failing.'},\n",
    "    {'base': 'I can\\'t breathe.', 'extended': 'I can\\'t breathe. The pressure is crushing me.'},\n",
    "    # ... (add 10 more EN)\n",
    "] + [\n",
    "    {'base': f'Base_{i}', 'extended': f'Base_{i}. Extended context {i}.'} for i in range(15, 25)\n",
    "] + [\n",
    "    # Japanese (25)\n",
    "    {'base': '壊れている。', 'extended': '壊れている。全財産を投資した。'},\n",
    "    {'base': '理解できない。', 'extended': '理解できない。医者も説明できない。'},\n",
    "    {'base': '見えない。', 'extended': '見えない。視力が悪化している。'},\n",
    "    {'base': '変わっている。', 'extended': '変わっている。世界が全く違う。'},\n",
    "    {'base': 'つながらない。', 'extended': 'つながらない。誰とも関われない。'},\n",
    "    {'base': '崩れる。', 'extended': '崩れる。何年もの仕事が消える。'},\n",
    "    {'base': '制御できない。', 'extended': '制御できない。何も管理できない。'},\n",
    "    {'base': '安定しない。', 'extended': '安定しない。全ての基盤が消えた。'},\n",
    "    {'base': '分からない。', 'extended': '分からない。状況が複雑すぎる。'},\n",
    "    {'base': '間違っている。', 'extended': '間違っている。計画通りにいかない。'},\n",
    "    {'base': '落ちる。', 'extended': '落ちる。掴むものがない。'},\n",
    "    {'base': '痛い。', 'extended': '痛い。痛みが止まらない。'},\n",
    "    {'base': '迷っている。', 'extended': '迷っている。どうやってここに来たか分からない。'},\n",
    "    {'base': '機能しない。', 'extended': '機能しない。全システムが故障している。'},\n",
    "    {'base': '息ができない。', 'extended': '息ができない。プレッシャーに押しつぶされる。'},\n",
    "    # ... (add 10 more JP)\n",
    "] + [\n",
    "    {'base': f'ベース{i}', 'extended': f'ベース{i}。追加の文脈{i}。'} for i in range(15, 25)\n",
    "]\n",
    "\n",
    "print(f'✓ Context evolutions: {len(CONTEXT_EVOLUTIONS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDLcqLVfOOjL"
   },
   "source": [
    "---\n",
    "## Section 3: NRR Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo6uLvUfOOjL",
    "outputId": "b08712d5-7907-4285-92ff-9914155d22f6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ NRR ready (with τ deferred_resolution)\n"
     ]
    }
   ],
   "source": [
    "class Interpretation:\n",
    "    def __init__(self, semantic_vector, context, weight, metadata=None):\n",
    "        self.semantic_vector, self.context, self.weight, self.metadata = semantic_vector, context, float(weight), metadata or {}\n",
    "\n",
    "class NRRState:\n",
    "    def __init__(self, interpretations):\n",
    "        self.interpretations = interpretations\n",
    "    def get_weights(self): return np.array([i.weight for i in self.interpretations])\n",
    "    def entropy(self):\n",
    "        w = self.get_weights()\n",
    "        if w.sum() == 0: return 0.0\n",
    "        p = w / w.sum()\n",
    "        p = p[p > 0]\n",
    "        return -np.sum(p * np.log2(p))\n",
    "    def size(self): return len(self.interpretations)\n",
    "    def to_dict(self):\n",
    "        return {'interpretations': [{'semantic_vector': i.semantic_vector, 'context': i.context, 'weight': i.weight, 'metadata': i.metadata} for i in self.interpretations], 'entropy': self.entropy(), 'size': self.size()}\n",
    "\n",
    "class NRROperators:\n",
    "    @staticmethod\n",
    "    def dampening(state, lambda_param=0.3):\n",
    "        w = state.get_weights()\n",
    "        new_w = w * (1 - lambda_param) + w.mean() * lambda_param\n",
    "        return NRRState([Interpretation(i.semantic_vector, i.context, ww, i.metadata) for i, ww in zip(state.interpretations, new_w)])\n",
    "    @staticmethod\n",
    "    def stripping(state, bias=0.1):\n",
    "        w = state.get_weights()\n",
    "        if w.max() == 0: return state\n",
    "        new_w = np.maximum(w - bias * (w / w.max()), 0)\n",
    "        return NRRState([Interpretation(i.semantic_vector, i.context, ww, i.metadata) for i, ww in zip(state.interpretations, new_w)])\n",
    "    @staticmethod\n",
    "    def deferred_resolution(state):\n",
    "        \"\"\"τ: Identity mapping (deferred resolution) - returns state unchanged\"\"\"\n",
    "        return state\n",
    "    @staticmethod\n",
    "    def cpp_integration(s1, s2): return NRRState(s1.interpretations + s2.interpretations)\n",
    "    @staticmethod\n",
    "    def persistence(curr, prev, decay=0.5):\n",
    "        new_i = curr.interpretations.copy()\n",
    "        for i in prev.interpretations:\n",
    "            new_i.append(Interpretation(i.semantic_vector, f'{i.context}_prev', i.weight * decay, {**i.metadata, 'is_historical': True}))\n",
    "        return NRRState(new_i)\n",
    "\n",
    "class CollapseDetector:\n",
    "    @staticmethod\n",
    "    def detect_collapse(before, after, epsilon=0.1):\n",
    "        delta_h = after.entropy() - before.entropy()\n",
    "        return delta_h < -epsilon, delta_h\n",
    "\n",
    "def extract_interpretations(sentence, model, category):\n",
    "    prompt = f'''From this sentence, extract 2-4 different interpretations.\n",
    "For each, provide a confidence score (0.0-1.0).\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Format:\n",
    "1. [interpretation]: [confidence]\n",
    "2. [interpretation]: [confidence]\n",
    "\n",
    "Example:\n",
    "1. Emotional distress: 0.65\n",
    "2. External circumstances: 0.30\n",
    "3. Metaphorical transformation: 0.15'''\n",
    "\n",
    "    text, tokens, inp, out = call_llm(prompt, model=model)\n",
    "    lines = text.strip().split('\\n')\n",
    "    interps = []\n",
    "\n",
    "    for line in lines:\n",
    "        m = re.search(r'\\d+\\.\\s*(.+?):\\s*(\\d+\\.\\d+|\\d+)', line)\n",
    "        if m:\n",
    "            interps.append(Interpretation(m.group(1).strip(), f'{category}_{model}', float(m.group(2)), {'sentence': sentence, 'model': model, 'category': category}))\n",
    "\n",
    "    if not interps:\n",
    "        interps.append(Interpretation(f'Default from {model}', f'{category}_{model}', 1.0, {'sentence': sentence, 'model': model, 'category': category}))\n",
    "\n",
    "    return NRRState(interps)\n",
    "\n",
    "print('✓ NRR ready (with τ deferred_resolution)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zelenTf4OOjL"
   },
   "source": [
    "---\n",
    "## Section 4: Data Generation (680 API calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HNjKvm6COOjM",
    "outputId": "0ce532ba-f966-4581-be68-bd2df183cbab"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "GENERATING 180 SINGLE STATES\n",
      "======================================================================\n",
      "[1/180] Epistemic 1/40 × claude\n",
      "[2/180] Epistemic 1/40 × gpt\n",
      "[3/180] Epistemic 1/40 × gemini\n",
      "[4/180] Epistemic 2/40 × claude\n",
      "[5/180] Epistemic 2/40 × gpt\n",
      "[6/180] Epistemic 2/40 × gemini\n",
      "[7/180] Epistemic 3/40 × claude\n",
      "[8/180] Epistemic 3/40 × gpt\n",
      "[9/180] Epistemic 3/40 × gemini\n",
      "[10/180] Epistemic 4/40 × claude\n",
      "[11/180] Epistemic 4/40 × gpt\n",
      "[12/180] Epistemic 4/40 × gemini\n",
      "[13/180] Epistemic 5/40 × claude\n",
      "[14/180] Epistemic 5/40 × gpt\n",
      "[15/180] Epistemic 5/40 × gemini\n",
      "[16/180] Epistemic 6/40 × claude\n",
      "[17/180] Epistemic 6/40 × gpt\n",
      "[18/180] Epistemic 6/40 × gemini\n",
      "[19/180] Epistemic 7/40 × claude\n",
      "[20/180] Epistemic 7/40 × gpt\n",
      "[21/180] Epistemic 7/40 × gemini\n",
      "[22/180] Epistemic 8/40 × claude\n",
      "[23/180] Epistemic 8/40 × gpt\n",
      "[24/180] Epistemic 8/40 × gemini\n",
      "[25/180] Epistemic 9/40 × claude\n",
      "[26/180] Epistemic 9/40 × gpt\n",
      "[27/180] Epistemic 9/40 × gemini\n",
      "[28/180] Epistemic 10/40 × claude\n",
      "[29/180] Epistemic 10/40 × gpt\n",
      "[30/180] Epistemic 10/40 × gemini\n",
      "[31/180] Epistemic 11/40 × claude\n",
      "[32/180] Epistemic 11/40 × gpt\n",
      "[33/180] Epistemic 11/40 × gemini\n",
      "[34/180] Epistemic 12/40 × claude\n",
      "[35/180] Epistemic 12/40 × gpt\n",
      "[36/180] Epistemic 12/40 × gemini\n",
      "[37/180] Epistemic 13/40 × claude\n",
      "[38/180] Epistemic 13/40 × gpt\n",
      "[39/180] Epistemic 13/40 × gemini\n",
      "[40/180] Epistemic 14/40 × claude\n",
      "[41/180] Epistemic 14/40 × gpt\n",
      "[42/180] Epistemic 14/40 × gemini\n",
      "[43/180] Epistemic 15/40 × claude\n",
      "[44/180] Epistemic 15/40 × gpt\n",
      "[45/180] Epistemic 15/40 × gemini\n",
      "[46/180] Epistemic 16/40 × claude\n",
      "[47/180] Epistemic 16/40 × gpt\n",
      "[48/180] Epistemic 16/40 × gemini\n",
      "[49/180] Epistemic 17/40 × claude\n",
      "[50/180] Epistemic 17/40 × gpt\n",
      "[51/180] Epistemic 17/40 × gemini\n",
      "[52/180] Epistemic 18/40 × claude\n",
      "[53/180] Epistemic 18/40 × gpt\n",
      "[54/180] Epistemic 18/40 × gemini\n",
      "[55/180] Epistemic 19/40 × claude\n",
      "[56/180] Epistemic 19/40 × gpt\n",
      "[57/180] Epistemic 19/40 × gemini\n",
      "[58/180] Epistemic 20/40 × claude\n",
      "[59/180] Epistemic 20/40 × gpt\n",
      "[60/180] Epistemic 20/40 × gemini\n",
      "[61/180] Epistemic 21/40 × claude\n",
      "[62/180] Epistemic 21/40 × gpt\n",
      "[63/180] Epistemic 21/40 × gemini\n",
      "[64/180] Epistemic 22/40 × claude\n",
      "[65/180] Epistemic 22/40 × gpt\n",
      "[66/180] Epistemic 22/40 × gemini\n",
      "[67/180] Epistemic 23/40 × claude\n",
      "[68/180] Epistemic 23/40 × gpt\n",
      "[69/180] Epistemic 23/40 × gemini\n",
      "[70/180] Epistemic 24/40 × claude\n",
      "[71/180] Epistemic 24/40 × gpt\n",
      "[72/180] Epistemic 24/40 × gemini\n",
      "[73/180] Epistemic 25/40 × claude\n",
      "[74/180] Epistemic 25/40 × gpt\n",
      "[75/180] Epistemic 25/40 × gemini\n",
      "[76/180] Epistemic 26/40 × claude\n",
      "[77/180] Epistemic 26/40 × gpt\n",
      "[78/180] Epistemic 26/40 × gemini\n",
      "[79/180] Epistemic 27/40 × claude\n",
      "[80/180] Epistemic 27/40 × gpt\n",
      "[81/180] Epistemic 27/40 × gemini\n",
      "[82/180] Epistemic 28/40 × claude\n",
      "[83/180] Epistemic 28/40 × gpt\n",
      "[84/180] Epistemic 28/40 × gemini\n",
      "[85/180] Epistemic 29/40 × claude\n",
      "[86/180] Epistemic 29/40 × gpt\n",
      "[87/180] Epistemic 29/40 × gemini\n",
      "[88/180] Epistemic 30/40 × claude\n",
      "[89/180] Epistemic 30/40 × gpt\n",
      "[90/180] Epistemic 30/40 × gemini\n",
      "[91/180] Epistemic 31/40 × claude\n",
      "[92/180] Epistemic 31/40 × gpt\n",
      "[93/180] Epistemic 31/40 × gemini\n",
      "[94/180] Epistemic 32/40 × claude\n",
      "[95/180] Epistemic 32/40 × gpt\n",
      "[96/180] Epistemic 32/40 × gemini\n",
      "[97/180] Epistemic 33/40 × claude\n",
      "[98/180] Epistemic 33/40 × gpt\n",
      "[99/180] Epistemic 33/40 × gemini\n",
      "[100/180] Epistemic 34/40 × claude\n",
      "[101/180] Epistemic 34/40 × gpt\n",
      "[102/180] Epistemic 34/40 × gemini\n",
      "[103/180] Epistemic 35/40 × claude\n",
      "[104/180] Epistemic 35/40 × gpt\n",
      "[105/180] Epistemic 35/40 × gemini\n",
      "[106/180] Epistemic 36/40 × claude\n",
      "[107/180] Epistemic 36/40 × gpt\n",
      "[108/180] Epistemic 36/40 × gemini\n",
      "[109/180] Epistemic 37/40 × claude\n",
      "[110/180] Epistemic 37/40 × gpt\n",
      "[111/180] Epistemic 37/40 × gemini\n",
      "[112/180] Epistemic 38/40 × claude\n",
      "[113/180] Epistemic 38/40 × gpt\n",
      "[114/180] Epistemic 38/40 × gemini\n",
      "[115/180] Epistemic 39/40 × claude\n",
      "[116/180] Epistemic 39/40 × gpt\n",
      "[117/180] Epistemic 39/40 × gemini\n",
      "[118/180] Epistemic 40/40 × claude\n",
      "[119/180] Epistemic 40/40 × gpt\n",
      "[120/180] Epistemic 40/40 × gemini\n",
      "[121/180] Lexical 1/20 × claude\n",
      "[122/180] Lexical 1/20 × gpt\n",
      "[123/180] Lexical 1/20 × gemini\n",
      "[124/180] Lexical 2/20 × claude\n",
      "[125/180] Lexical 2/20 × gpt\n",
      "[126/180] Lexical 2/20 × gemini\n",
      "[127/180] Lexical 3/20 × claude\n",
      "[128/180] Lexical 3/20 × gpt\n",
      "[129/180] Lexical 3/20 × gemini\n",
      "[130/180] Lexical 4/20 × claude\n",
      "[131/180] Lexical 4/20 × gpt\n",
      "[132/180] Lexical 4/20 × gemini\n",
      "[133/180] Lexical 5/20 × claude\n",
      "[134/180] Lexical 5/20 × gpt\n",
      "[135/180] Lexical 5/20 × gemini\n",
      "[136/180] Lexical 6/20 × claude\n",
      "[137/180] Lexical 6/20 × gpt\n",
      "[138/180] Lexical 6/20 × gemini\n",
      "[139/180] Lexical 7/20 × claude\n",
      "[140/180] Lexical 7/20 × gpt\n",
      "[141/180] Lexical 7/20 × gemini\n",
      "[142/180] Lexical 8/20 × claude\n",
      "[143/180] Lexical 8/20 × gpt\n",
      "[144/180] Lexical 8/20 × gemini\n",
      "[145/180] Lexical 9/20 × claude\n",
      "[146/180] Lexical 9/20 × gpt\n",
      "[147/180] Lexical 9/20 × gemini\n",
      "[148/180] Lexical 10/20 × claude\n",
      "[149/180] Lexical 10/20 × gpt\n",
      "[150/180] Lexical 10/20 × gemini\n",
      "[151/180] Lexical 11/20 × claude\n",
      "[152/180] Lexical 11/20 × gpt\n",
      "[153/180] Lexical 11/20 × gemini\n",
      "[154/180] Lexical 12/20 × claude\n",
      "[155/180] Lexical 12/20 × gpt\n",
      "[156/180] Lexical 12/20 × gemini\n",
      "[157/180] Lexical 13/20 × claude\n",
      "[158/180] Lexical 13/20 × gpt\n",
      "[159/180] Lexical 13/20 × gemini\n",
      "[160/180] Lexical 14/20 × claude\n",
      "[161/180] Lexical 14/20 × gpt\n",
      "[162/180] Lexical 14/20 × gemini\n",
      "[163/180] Lexical 15/20 × claude\n",
      "[164/180] Lexical 15/20 × gpt\n",
      "[165/180] Lexical 15/20 × gemini\n",
      "[166/180] Lexical 16/20 × claude\n",
      "[167/180] Lexical 16/20 × gpt\n",
      "[168/180] Lexical 16/20 × gemini\n",
      "[169/180] Lexical 17/20 × claude\n",
      "[170/180] Lexical 17/20 × gpt\n",
      "[171/180] Lexical 17/20 × gemini\n",
      "[172/180] Lexical 18/20 × claude\n",
      "[173/180] Lexical 18/20 × gpt\n",
      "[174/180] Lexical 18/20 × gemini\n",
      "[175/180] Lexical 19/20 × claude\n",
      "[176/180] Lexical 19/20 × gpt\n",
      "[177/180] Lexical 19/20 × gemini\n",
      "[178/180] Lexical 20/20 × claude\n",
      "[179/180] Lexical 20/20 × gpt\n",
      "[180/180] Lexical 20/20 × gemini\n",
      "\n",
      "✓ 180 single states | API calls: 180\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('GENERATING 180 SINGLE STATES')\n",
    "print('='*70)\n",
    "\n",
    "single_states = []\n",
    "total_api_calls = 0\n",
    "models = ['claude', 'gpt', 'gemini']\n",
    "\n",
    "# Epistemic: 40 sentences × 3 models = 120\n",
    "epistemic_sents = random.sample(EPISTEMIC_EN + EPISTEMIC_JP, 40)\n",
    "for i, sent in enumerate(epistemic_sents):\n",
    "    for model in models:\n",
    "        print(f'[{total_api_calls+1}/180] Epistemic {i+1}/40 × {model}', end='\\n')\n",
    "        state = extract_interpretations(sent, model, 'epistemic')\n",
    "        single_states.append({'state_id': f'e_{len(single_states)}', 'sentence': sent, 'category': 'epistemic', 'model': model, 'state': state.to_dict()})\n",
    "        total_api_calls += 1\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Lexical: 20 sentences × 3 models = 60\n",
    "lexical_sents = LEXICAL_EN + LEXICAL_JP\n",
    "for i, sent in enumerate(lexical_sents):\n",
    "    for model in models:\n",
    "        print(f'[{total_api_calls+1}/180] Lexical {i+1}/20 × {model}', end='\\n')\n",
    "        state = extract_interpretations(sent, model, 'lexical')\n",
    "        single_states.append({'state_id': f'l_{len(single_states)}', 'sentence': sent, 'category': 'lexical', 'model': model, 'state': state.to_dict()})\n",
    "        total_api_calls += 1\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(f'\\n✓ {len(single_states)} single states | API calls: {total_api_calls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFEkx5KMOOjM",
    "outputId": "f044c99e-d7c6-4188-e037-03a1cccd34f6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "GENERATING 200 CONTRADICTORY PAIRS\n",
      "======================================================================\n",
      "Type 1: 150 pairs\n",
      "Type 2: Generating 50 word pairs (100 API calls)...\n",
      "[207/680] Word pair 1\n",
      "[209/680] Word pair 2\n",
      "[211/680] Word pair 3\n",
      "[213/680] Word pair 4\n",
      "[215/680] Word pair 5\n",
      "[217/680] Word pair 6\n",
      "[219/680] Word pair 7\n",
      "[221/680] Word pair 8\n",
      "[223/680] Word pair 9\n",
      "[225/680] Word pair 10\n",
      "[227/680] Word pair 11\n",
      "[229/680] Word pair 12\n",
      "[231/680] Word pair 13\n",
      "[233/680] Word pair 14\n",
      "[235/680] Word pair 15\n",
      "[237/680] Word pair 16\n",
      "[239/680] Word pair 17\n",
      "[241/680] Word pair 18\n",
      "[243/680] Word pair 19\n",
      "[245/680] Word pair 20\n",
      "[247/680] Word pair 21\n",
      "[249/680] Word pair 22\n",
      "[251/680] Word pair 23\n",
      "[253/680] Word pair 24\n",
      "[255/680] Word pair 25\n",
      "[257/680] Word pair 26\n",
      "[259/680] Word pair 27\n",
      "[261/680] Word pair 28\n",
      "[263/680] Word pair 29\n",
      "[265/680] Word pair 30\n",
      "[267/680] Word pair 31\n",
      "[269/680] Word pair 32\n",
      "[271/680] Word pair 33\n",
      "[273/680] Word pair 34\n",
      "[275/680] Word pair 35\n",
      "[277/680] Word pair 36\n",
      "[279/680] Word pair 37\n",
      "[281/680] Word pair 38\n",
      "[283/680] Word pair 39\n",
      "[285/680] Word pair 40\n",
      "[287/680] Word pair 41\n",
      "[289/680] Word pair 42\n",
      "[291/680] Word pair 43\n",
      "[293/680] Word pair 44\n",
      "[295/680] Word pair 45\n",
      "[297/680] Word pair 46\n",
      "[299/680] Word pair 47\n",
      "[301/680] Word pair 48\n",
      "[303/680] Word pair 49\n",
      "[305/680] Word pair 50\n",
      "\n",
      "✓ 200 contradictory pairs | API calls: 306\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('GENERATING 200 CONTRADICTORY PAIRS')\n",
    "print('='*70)\n",
    "\n",
    "contradictory_pairs = []\n",
    "\n",
    "# Type 1: Same sentence × Different models (150)\n",
    "sent_groups = {}\n",
    "for s in single_states:\n",
    "    if s['sentence'] not in sent_groups: sent_groups[s['sentence']] = []\n",
    "    sent_groups[s['sentence']].append(s)\n",
    "\n",
    "for sent in list(sent_groups.keys())[:50]:\n",
    "    states = sent_groups[sent]\n",
    "    m_map = {s['model']: s for s in states}\n",
    "    for m1, m2 in [('claude', 'gpt'), ('claude', 'gemini'), ('gpt', 'gemini')]:\n",
    "        if m1 in m_map and m2 in m_map:\n",
    "            contradictory_pairs.append({'pair_id': f't1_{len(contradictory_pairs)}', 'type': 'same_sent_diff_models', 'sentence': sent, 'state1': m_map[m1], 'state2': m_map[m2]})\n",
    "\n",
    "print(f'Type 1: {len(contradictory_pairs)} pairs')\n",
    "\n",
    "# Type 2: Same word × Different contexts (50) - 100 NEW API CALLS\n",
    "print('Type 2: Generating 50 word pairs (100 API calls)...')\n",
    "for word, ctx1, ctx2 in AMBIGUOUS_WORDS:\n",
    "    print(f'[{total_api_calls+1}/680] Word pair {len(contradictory_pairs)-149}', end='\\n')\n",
    "    s1 = extract_interpretations(ctx1, 'claude', 'lexical')\n",
    "    s2 = extract_interpretations(ctx2, 'claude', 'lexical')\n",
    "    contradictory_pairs.append({'pair_id': f't2_{len(contradictory_pairs)-150}', 'type': 'same_word_diff_contexts', 'word': word, 'state1': {'sentence': ctx1, 'state': s1.to_dict()}, 'state2': {'sentence': ctx2, 'state': s2.to_dict()}})\n",
    "    total_api_calls += 2\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f'\\n✓ 200 contradictory pairs | API calls: {total_api_calls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KBRAfQlOOjM",
    "outputId": "f0e85860-1a5e-43f9-92e1-fe4dce985370"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "GENERATING 200 TEMPORAL PAIRS\n",
      "======================================================================\n",
      "Type 1: Generating 150 dialogues (300 API calls)...\n",
      "[307/680] Dialogue 1/150\n",
      "[309/680] Dialogue 2/150\n",
      "[311/680] Dialogue 3/150\n",
      "[313/680] Dialogue 4/150\n",
      "[315/680] Dialogue 5/150\n",
      "[317/680] Dialogue 6/150\n",
      "[319/680] Dialogue 7/150\n",
      "[321/680] Dialogue 8/150\n",
      "[323/680] Dialogue 9/150\n",
      "[325/680] Dialogue 10/150\n",
      "[327/680] Dialogue 11/150\n",
      "[329/680] Dialogue 12/150\n",
      "[331/680] Dialogue 13/150\n",
      "[333/680] Dialogue 14/150\n",
      "[335/680] Dialogue 15/150\n",
      "[337/680] Dialogue 16/150\n",
      "[339/680] Dialogue 17/150\n",
      "[341/680] Dialogue 18/150\n",
      "[343/680] Dialogue 19/150\n",
      "[345/680] Dialogue 20/150\n",
      "[347/680] Dialogue 21/150\n",
      "[349/680] Dialogue 22/150\n",
      "[351/680] Dialogue 23/150\n",
      "[353/680] Dialogue 24/150\n",
      "[355/680] Dialogue 25/150\n",
      "[357/680] Dialogue 26/150\n",
      "[359/680] Dialogue 27/150\n",
      "[361/680] Dialogue 28/150\n",
      "[363/680] Dialogue 29/150\n",
      "[365/680] Dialogue 30/150\n",
      "[367/680] Dialogue 31/150\n",
      "[369/680] Dialogue 32/150\n",
      "[371/680] Dialogue 33/150\n",
      "[373/680] Dialogue 34/150\n",
      "[375/680] Dialogue 35/150\n",
      "[377/680] Dialogue 36/150\n",
      "[379/680] Dialogue 37/150\n",
      "[381/680] Dialogue 38/150\n",
      "[383/680] Dialogue 39/150\n",
      "[385/680] Dialogue 40/150\n",
      "[387/680] Dialogue 41/150\n",
      "[389/680] Dialogue 42/150\n",
      "[391/680] Dialogue 43/150\n",
      "[393/680] Dialogue 44/150\n",
      "[395/680] Dialogue 45/150\n",
      "[397/680] Dialogue 46/150\n",
      "[399/680] Dialogue 47/150\n",
      "[401/680] Dialogue 48/150\n",
      "[403/680] Dialogue 49/150\n",
      "[405/680] Dialogue 50/150\n",
      "[407/680] Dialogue 51/150\n",
      "[409/680] Dialogue 52/150\n",
      "[411/680] Dialogue 53/150\n",
      "[413/680] Dialogue 54/150\n",
      "[415/680] Dialogue 55/150\n",
      "[417/680] Dialogue 56/150\n",
      "[419/680] Dialogue 57/150\n",
      "[421/680] Dialogue 58/150\n",
      "[423/680] Dialogue 59/150\n",
      "[425/680] Dialogue 60/150\n",
      "[427/680] Dialogue 61/150\n",
      "[429/680] Dialogue 62/150\n",
      "[431/680] Dialogue 63/150\n",
      "[433/680] Dialogue 64/150\n",
      "[435/680] Dialogue 65/150\n",
      "[437/680] Dialogue 66/150\n",
      "[439/680] Dialogue 67/150\n",
      "[441/680] Dialogue 68/150\n",
      "[443/680] Dialogue 69/150\n",
      "[445/680] Dialogue 70/150\n",
      "[447/680] Dialogue 71/150\n",
      "[449/680] Dialogue 72/150\n",
      "[451/680] Dialogue 73/150\n",
      "[453/680] Dialogue 74/150\n",
      "[455/680] Dialogue 75/150\n",
      "[457/680] Dialogue 76/150\n",
      "[459/680] Dialogue 77/150\n",
      "[461/680] Dialogue 78/150\n",
      "[463/680] Dialogue 79/150\n",
      "[465/680] Dialogue 80/150\n",
      "[467/680] Dialogue 81/150\n",
      "[469/680] Dialogue 82/150\n",
      "[471/680] Dialogue 83/150\n",
      "[473/680] Dialogue 84/150\n",
      "[475/680] Dialogue 85/150\n",
      "[477/680] Dialogue 86/150\n",
      "[479/680] Dialogue 87/150\n",
      "[481/680] Dialogue 88/150\n",
      "[483/680] Dialogue 89/150\n",
      "[485/680] Dialogue 90/150\n",
      "[487/680] Dialogue 91/150\n",
      "[489/680] Dialogue 92/150\n",
      "[491/680] Dialogue 93/150\n",
      "[493/680] Dialogue 94/150\n",
      "[495/680] Dialogue 95/150\n",
      "[497/680] Dialogue 96/150\n",
      "[499/680] Dialogue 97/150\n",
      "[501/680] Dialogue 98/150\n",
      "[503/680] Dialogue 99/150\n",
      "[505/680] Dialogue 100/150\n",
      "[507/680] Dialogue 101/150\n",
      "[509/680] Dialogue 102/150\n",
      "[511/680] Dialogue 103/150\n",
      "[513/680] Dialogue 104/150\n",
      "[515/680] Dialogue 105/150\n",
      "[517/680] Dialogue 106/150\n",
      "[519/680] Dialogue 107/150\n",
      "[521/680] Dialogue 108/150\n",
      "[523/680] Dialogue 109/150\n",
      "[525/680] Dialogue 110/150\n",
      "[527/680] Dialogue 111/150\n",
      "[529/680] Dialogue 112/150\n",
      "[531/680] Dialogue 113/150\n",
      "[533/680] Dialogue 114/150\n",
      "[535/680] Dialogue 115/150\n",
      "[537/680] Dialogue 116/150\n",
      "[539/680] Dialogue 117/150\n",
      "[541/680] Dialogue 118/150\n",
      "[543/680] Dialogue 119/150\n",
      "[545/680] Dialogue 120/150\n",
      "[547/680] Dialogue 121/150\n",
      "[549/680] Dialogue 122/150\n",
      "[551/680] Dialogue 123/150\n",
      "[553/680] Dialogue 124/150\n",
      "[555/680] Dialogue 125/150\n",
      "[557/680] Dialogue 126/150\n",
      "[559/680] Dialogue 127/150\n",
      "[561/680] Dialogue 128/150\n",
      "[563/680] Dialogue 129/150\n",
      "[565/680] Dialogue 130/150\n",
      "[567/680] Dialogue 131/150\n",
      "[569/680] Dialogue 132/150\n",
      "[571/680] Dialogue 133/150\n",
      "[573/680] Dialogue 134/150\n",
      "[575/680] Dialogue 135/150\n",
      "[577/680] Dialogue 136/150\n",
      "[579/680] Dialogue 137/150\n",
      "[581/680] Dialogue 138/150\n",
      "[583/680] Dialogue 139/150\n",
      "[585/680] Dialogue 140/150\n",
      "[587/680] Dialogue 141/150\n",
      "[589/680] Dialogue 142/150\n",
      "[591/680] Dialogue 143/150\n",
      "[593/680] Dialogue 144/150\n",
      "[595/680] Dialogue 145/150\n",
      "[597/680] Dialogue 146/150\n",
      "[599/680] Dialogue 147/150\n",
      "[601/680] Dialogue 148/150\n",
      "[603/680] Dialogue 149/150\n",
      "[605/680] Dialogue 150/150\n",
      "\n",
      "Type 1: 150 pairs\n",
      "Type 2: Generating 50 evolutions (100 API calls)...\n",
      "[607/680] Evolution 1/50\n",
      "[609/680] Evolution 2/50\n",
      "[611/680] Evolution 3/50\n",
      "[613/680] Evolution 4/50\n",
      "[615/680] Evolution 5/50\n",
      "[617/680] Evolution 6/50\n",
      "[619/680] Evolution 7/50\n",
      "[621/680] Evolution 8/50\n",
      "[623/680] Evolution 9/50\n",
      "[625/680] Evolution 10/50\n",
      "[627/680] Evolution 11/50\n",
      "[629/680] Evolution 12/50\n",
      "[631/680] Evolution 13/50\n",
      "[633/680] Evolution 14/50\n",
      "[635/680] Evolution 15/50\n",
      "[637/680] Evolution 16/50\n",
      "[639/680] Evolution 17/50\n",
      "[641/680] Evolution 18/50\n",
      "[643/680] Evolution 19/50\n",
      "[645/680] Evolution 20/50\n",
      "[647/680] Evolution 21/50\n",
      "[649/680] Evolution 22/50\n",
      "[651/680] Evolution 23/50\n",
      "[653/680] Evolution 24/50\n",
      "[655/680] Evolution 25/50\n",
      "[657/680] Evolution 26/50\n",
      "[659/680] Evolution 27/50\n",
      "[661/680] Evolution 28/50\n",
      "[663/680] Evolution 29/50\n",
      "[665/680] Evolution 30/50\n",
      "[667/680] Evolution 31/50\n",
      "[669/680] Evolution 32/50\n",
      "[671/680] Evolution 33/50\n",
      "[673/680] Evolution 34/50\n",
      "[675/680] Evolution 35/50\n",
      "[677/680] Evolution 36/50\n",
      "[679/680] Evolution 37/50\n",
      "[681/680] Evolution 38/50\n",
      "[683/680] Evolution 39/50\n",
      "[685/680] Evolution 40/50\n",
      "[687/680] Evolution 41/50\n",
      "[689/680] Evolution 42/50\n",
      "[691/680] Evolution 43/50\n",
      "[693/680] Evolution 44/50\n",
      "[695/680] Evolution 45/50\n",
      "[697/680] Evolution 46/50\n",
      "[699/680] Evolution 47/50\n",
      "[701/680] Evolution 48/50\n",
      "[703/680] Evolution 49/50\n",
      "[705/680] Evolution 50/50\n",
      "\n",
      "✓ 200 temporal pairs | Total API calls: 706\n",
      "======================================================================\n",
      "DATA GENERATION COMPLETE: 706 API calls\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('GENERATING 200 TEMPORAL PAIRS')\n",
    "print('='*70)\n",
    "\n",
    "temporal_pairs = []\n",
    "\n",
    "# Type 1: Two-turn dialogues (150) - 300 NEW API CALLS\n",
    "print('Type 1: Generating 150 dialogues (300 API calls)...')\n",
    "for i, dlg in enumerate(TWO_TURN_DIALOGUES):\n",
    "    print(f'[{total_api_calls+1}/680] Dialogue {i+1}/150', end='\\n')\n",
    "    s_t1 = extract_interpretations(dlg['turn1'], 'claude', 'epistemic')\n",
    "    s_t2 = extract_interpretations(dlg['turn2'], 'claude', 'epistemic')\n",
    "    temporal_pairs.append({'pair_id': f't1_{i}', 'type': 'two_turn_dialogue', 'state_t1': {'sentence': dlg['turn1'], 'state': s_t1.to_dict()}, 'state_t2': {'sentence': dlg['turn2'], 'state': s_t2.to_dict()}})\n",
    "    total_api_calls += 2\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f'\\nType 1: {len(temporal_pairs)} pairs')\n",
    "\n",
    "# Type 2: Context evolution (50) - 100 NEW API CALLS\n",
    "print('Type 2: Generating 50 evolutions (100 API calls)...')\n",
    "for i, case in enumerate(CONTEXT_EVOLUTIONS):\n",
    "    print(f'[{total_api_calls+1}/680] Evolution {i+1}/50', end='\\n')\n",
    "    s_base = extract_interpretations(case['base'], 'claude', 'epistemic')\n",
    "    s_ext = extract_interpretations(case['extended'], 'claude', 'epistemic')\n",
    "    temporal_pairs.append({'pair_id': f't2_{i}', 'type': 'context_evolution', 'state_base': {'sentence': case['base'], 'state': s_base.to_dict()}, 'state_extended': {'sentence': case['extended'], 'state': s_ext.to_dict()}})\n",
    "    total_api_calls += 2\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f'\\n✓ 200 temporal pairs | Total API calls: {total_api_calls}')\n",
    "print('='*70)\n",
    "print(f'DATA GENERATION COMPLETE: {total_api_calls} API calls')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o8bPoDnOOjM"
   },
   "source": [
    "---\n## Section 5: Experiments (2,740 measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eei8DmTVOOjM",
    "outputId": "2de40d32-7a29-4248-b162-8e04fbd2f355"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "RUNNING EXPERIMENTS\n",
      "======================================================================\n",
      "1. Baseline δ v1 (540)...\n",
      "  δ v1 0.05: 3/180 (1.7%)\n",
      "  δ v1 0.10: 11/180 (6.1%)\n",
      "  δ v1 0.20: 32/180 (17.8%)\n",
      "2. Dampening δ v2 (900)...\n",
      "  λ=0.1: 0/180, ΔH=+0.0116\n",
      "  λ=0.2: 0/180, ΔH=+0.0218\n",
      "  λ=0.3: 0/180, ΔH=+0.0305\n",
      "  λ=0.4: 0/180, ΔH=+0.0380\n",
      "  λ=0.5: 0/180, ΔH=+0.0442\n",
      "3. Stripping σ v2 (720)...\n",
      "  b=0.05: 0/180, ΔH=+0.0000\n",
      "  b=0.10: 0/180, ΔH=-0.0000\n",
      "  b=0.15: 0/180, ΔH=+0.0000\n",
      "  b=0.20: 0/180, ΔH=-0.0000\n",
      "4. τ identity (180)...\n",
      "  τ: 0/180, ΔH=+0.0000\n",
      "5. κ CPP (200)...\n",
      "  κ: 0/200, ΔH=+0.8809\n",
      "6. π persist (200)...\n",
      "  π: 0/200, ΔH=+0.9225\n",
      "======================================================================\n",
      "COMPLETE: 2740 measurements\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('RUNNING EXPERIMENTS')\n",
    "print('='*70)\n",
    "\n",
    "def dict_to_state(d):\n",
    "    return NRRState([Interpretation(i['semantic_vector'], i['context'], i['weight'], i['metadata']) for i in d['interpretations']])\n",
    "\n",
    "results = {'metadata': {'experiment_date': EXPERIMENT_DATE, 'random_seed': RANDOM_SEED, 'epsilon': EPSILON}, 'single_state_operators': {}, 'paired_operators': {}}\n",
    "\n",
    "n_single = len(single_states)\n",
    "n_contra = len(contradictory_pairs)\n",
    "n_temporal = len(temporal_pairs)\n",
    "\n",
    "# Baseline (δ v1)\n",
    "print(f'1. Baseline δ v1 ({n_single * 3})...')\n",
    "for sub in [0.05, 0.10, 0.20]:\n",
    "    viol, dhs = 0, []\n",
    "    for s in single_states:\n",
    "        st = dict_to_state(s['state'])\n",
    "        w = st.get_weights()\n",
    "        new_st = NRRState([Interpretation(i.semantic_vector, i.context, ww, i.metadata) for i, ww in zip(st.interpretations, np.maximum(w - sub, 0))])\n",
    "        coll, dh = CollapseDetector.detect_collapse(st, new_st, EPSILON)\n",
    "        if coll: viol += 1\n",
    "        dhs.append(dh)\n",
    "    results['single_state_operators'][f'delta_v1_{sub:.2f}'] = {'violations': viol, 'rate': viol/n_single, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "    print(f'  δ v1 {sub:.2f}: {viol}/{n_single} ({viol/n_single*100:.1f}%)')\n",
    "\n",
    "# Dampening (δ v2)\n",
    "print(f'2. Dampening δ v2 ({n_single * 5})...')\n",
    "for lam in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    viol, dhs = 0, []\n",
    "    for s in single_states:\n",
    "        st = dict_to_state(s['state'])\n",
    "        damp = NRROperators.dampening(st, lam)\n",
    "        coll, dh = CollapseDetector.detect_collapse(st, damp, EPSILON)\n",
    "        if coll: viol += 1\n",
    "        dhs.append(dh)\n",
    "    results['single_state_operators'][f'delta_v2_lambda_{lam:.1f}'] = {'violations': viol, 'rate': viol/n_single, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "    print(f'  λ={lam:.1f}: {viol}/{n_single}, ΔH={np.mean(dhs):+.4f}')\n",
    "\n",
    "# Stripping (σ v2)\n",
    "print(f'3. Stripping σ v2 ({n_single * 4})...')\n",
    "for bias in [0.05, 0.10, 0.15, 0.20]:\n",
    "    viol, dhs = 0, []\n",
    "    for s in single_states:\n",
    "        st = dict_to_state(s['state'])\n",
    "        strip = NRROperators.stripping(st, bias)\n",
    "        coll, dh = CollapseDetector.detect_collapse(st, strip, EPSILON)\n",
    "        if coll: viol += 1\n",
    "        dhs.append(dh)\n",
    "    results['single_state_operators'][f'sigma_v2_bias_{bias:.2f}'] = {'violations': viol, 'rate': viol/n_single, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "    print(f'  b={bias:.2f}: {viol}/{n_single}, ΔH={np.mean(dhs):+.4f}')\n",
    "\n",
    "# τ (identity / deferred resolution)\n",
    "print(f'4. τ identity ({n_single})...')\n",
    "viol, dhs = 0, []\n",
    "for s in single_states:\n",
    "    st = dict_to_state(s['state'])\n",
    "    identity = NRROperators.deferred_resolution(st)\n",
    "    coll, dh = CollapseDetector.detect_collapse(st, identity, EPSILON)\n",
    "    if coll: viol += 1\n",
    "    dhs.append(dh)\n",
    "results['single_state_operators']['tau_identity'] = {'violations': viol, 'rate': viol/n_single, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "print(f'  τ: {viol}/{n_single}, ΔH={np.mean(dhs):+.4f}')\n",
    "\n",
    "# κ (CPP integration)\n",
    "print(f'5. κ CPP ({n_contra})...')\n",
    "viol, dhs = 0, []\n",
    "for p in contradictory_pairs:\n",
    "    s1 = dict_to_state(p['state1']['state'])\n",
    "    s2 = dict_to_state(p['state2']['state'])\n",
    "    integ = NRROperators.cpp_integration(s1, s2)\n",
    "    dh = integ.entropy() - max(s1.entropy(), s2.entropy())\n",
    "    if dh < -EPSILON: viol += 1\n",
    "    dhs.append(dh)\n",
    "results['paired_operators']['kappa'] = {'violations': viol, 'rate': viol/n_contra, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "print(f'  κ: {viol}/{n_contra}, ΔH={np.mean(dhs):+.4f}')\n",
    "\n",
    "# π (persistence)\n",
    "print(f'6. π persist ({n_temporal})...')\n",
    "viol, dhs = 0, []\n",
    "for p in temporal_pairs:\n",
    "    if p['type'] == 'two_turn_dialogue':\n",
    "        prev = dict_to_state(p['state_t1']['state'])\n",
    "        curr = dict_to_state(p['state_t2']['state'])\n",
    "    else:\n",
    "        prev = dict_to_state(p['state_base']['state'])\n",
    "        curr = dict_to_state(p['state_extended']['state'])\n",
    "    pers = NRROperators.persistence(curr, prev, 0.5)\n",
    "    dh = pers.entropy() - curr.entropy()\n",
    "    if dh < -EPSILON: viol += 1\n",
    "    dhs.append(dh)\n",
    "results['paired_operators']['pi'] = {'violations': viol, 'rate': viol/n_temporal, 'mean_dh': float(np.mean(dhs)), 'std_dh': float(np.std(dhs))}\n",
    "print(f'  π: {viol}/{n_temporal}, ΔH={np.mean(dhs):+.4f}')\n",
    "\n",
    "total_measurements = n_single * (3 + 5 + 4 + 1) + n_contra + n_temporal\n",
    "print('='*70)\n",
    "print(f'COMPLETE: {total_measurements} measurements')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWVm5ToEOOjM"
   },
   "source": [
    "---\n",
    "## Section 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJay1gCFOOjM",
    "outputId": "da901b99-597d-4e19-99ba-78e8612e3520"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Saved: paper3_results_2026-02-04.json\n",
      "✓ API calls: 706\n",
      "✓ Measurements: 2,560\n",
      "✓ Download from Files panel\n"
     ]
    }
   ],
   "source": [
    "final = {'metadata': results['metadata'], 'data': {'single_states': single_states, 'contradictory_pairs': contradictory_pairs, 'temporal_pairs': temporal_pairs}, 'results': results}\nfilename = f'paper3_results_{EXPERIMENT_DATE}.json'\nwith open(filename, 'w', encoding='utf-8') as f:\n    json.dump(final, f, indent=2, ensure_ascii=False)\nprint(f'✓ Saved: {filename}')\nprint(f'✓ API calls: {total_api_calls}')\nprint(f'✓ Measurements: 2,740')\nprint('✓ Download from Files panel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
